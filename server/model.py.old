from const import LOADING_LOG
import importlib
#import importlib.util
import pickle
import os
import schemata
import torch

from collections import Iterable, OrderedDict
from junky import BaseConfig, to_device
from junky.layers import CharEmbeddingRNN, CharEmbeddingCNN


#from numba import cuda
#device = cuda.get_current_device()
#device.reset()
#cuda.select_device(1)
#cuda.close()
#tf.keras.backend.clear_session()
def garbage():
    gc.collect()
    #torch.cuda.empty_cache()

# (clf, format, device)
model = None

def load():
    config = schemata.config
    print('MODEL LOADING')
    print(config)
    global model

    globs = globals()

    def load_module(path):
        pkg_name = os.path.splitext(os.path.basename(path))[0]
        spec = importlib.util.spec_from_file_location(pkg_name, path)
        cls = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(cls)
        print('CLASS', cls)
        print(dir(cls))
        for var in dir(cls):
            if not var.startswith('__'):
                if var in globs:
                    raise ValueError(f'ERROR: The name {var} is reserved.')
                globs[var] = getattr(cls, var)

    model_import = config.model_import
    if os.path.isfile(model_import):
        load_class(config.model_import)
    elif os.path.isdir(model_import):
        for fn in os.listdir(model_import):
            fn = os.path.join(model_import, fn)
            if os.path.isfile(fn):
                load_module(fn)
    else:
        raise ValueError(
            f'ERROR: No such file or directory: "{model_import}".'
        )
    print('GLOBS_AFTER', globs)
    print('MODEL', Model)

    if config.model_format == schemata.FormatEnum.torch:
        new_model = torch.load(config.model_name, map_location='cpu') \
                         .to(config.model_device)
    else:
        raise ValueError(f'ERROR: Unknown model format {format}.')
    old_model = model if model else None
    model = (new_model, format, device)
    if old_model:
        clf, format, device = old_model
        if format == FormatEnum.torch:
            clf.cpu()
        del clf

def predict(data):
    with torch.no_grad():
        res = model[0](data)
    return res
